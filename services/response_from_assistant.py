
import openai
from utils.logger import setup_logger
from utils.api_utils import async_openai_error_handler
import config as cfg  # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å: OPENAI_API_KEY, ASSISTANT_ID, INSTRUCTION_ASSISTANT, MODELS_GPT

logger = setup_logger(__name__)

class ResponseAssistantError(Exception):
    """–ö–∞—Å—Ç–æ–º–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞."""
    pass

class ResponseAssistantAll:
    """Service for generating text using OpenAI GPT models."""

    def __init__(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é API-–∫–ª—é—á–∞."""
        self.validate_response_config()
        openai.api_key = cfg.OPENAI_API_KEY

    @staticmethod
    def validate_response_config():
        """Validates that required configuration for text generation is present.

                Raises:
                    ValueError: If API key or models are not configured.
                """
        if not cfg.OPENAI_API_KEY:
            raise ValueError("–ù–µ –∑–∞–¥–∞–Ω API-–∫–ª—é—á OpenAI.")
        if not hasattr(cfg, "MODELS_GPT") or not cfg.MODELS_GPT:
            raise ValueError("–ù–µ –∑–∞–¥–∞–Ω—ã –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ GPT.")

    @staticmethod
    def validate_model(model: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ª–∏ –≤—ã–±—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å.Args:
            model (str): The model name.

        Returns:
            bool: True if supported, False otherwise.
        """
        return model in cfg.MODELS_GPT

    async def text_generation(self, update, context, user_message: str, model: str = "gpt-4o") -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OpenAI GPT.Args:
            Generates text based on the user's message using OpenAI GPT.

        Args:
            update: Telegram update.
            context: Telegram context.
            user_message (str): The user's input.
            model (str, optional): The model to use. Defaults to "gpt-4o".

        Returns:
            str: Generated text with token usage statistics.

        Raises:
            ResponseAssistantError: If generation fails."""

        if not user_message or not isinstance(user_message, str):
            raise ValueError("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.")
        if len(user_message) > 16000:
            raise ValueError("–¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å 16000 —Å–∏–º–≤–æ–ª–æ–≤.")
        if not self.validate_model(model):
            raise ValueError(f"–í—ã–±—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å '{model}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.")

        status_message = await update.message.reply_text("‚è≥ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...")
        response = openai.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": user_message}],
            max_tokens=1000
        )
        input_tokens = response.usage.prompt_tokens
        output_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens
        generated_text = response.choices[0].message.content

        result_message = (
            f"{generated_text}\n\n"
            f"üîπ *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤:*\n"
            f"üì• –í—Ö–æ–¥—è—â–∏–µ: {input_tokens}\n"
            f"üì§ –ò—Å—Ö–æ–¥—è—â–∏–µ: {output_tokens}\n"
            f"üí∞ –í—Å–µ–≥–æ: {total_tokens}"
        )
        await status_message.edit_text("‚úÖ –û—Ç–≤–µ—Ç –≥–æ—Ç–æ–≤!")
        logger.info(
            f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {generated_text[:50]}... (–í—Ö–æ–¥—è—â–∏–µ: {input_tokens}, "
            f"–ò—Å—Ö–æ–¥—è—â–∏–µ: {output_tokens}, –í—Å–µ–≥–æ: {total_tokens})"
        )
        return result_message



